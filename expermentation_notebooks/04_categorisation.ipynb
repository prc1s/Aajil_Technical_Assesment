{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34aa8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf97b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hh/MLops/ajjil_technical_task'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e9776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datascience.constants import *\n",
    "from src.datascience.utils.common import (read_yaml, create_directories, load_json)\n",
    "from src.datascience import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd11786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hh/MLops/ajjil_technical_task/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5de9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CategorisationConfig:\n",
    "    root_dir: Path\n",
    "    source: Path\n",
    "    seeds: Path\n",
    "\n",
    "    model_name: str\n",
    "    batch_size: int\n",
    "    normalise: bool\n",
    "\n",
    "    use_pca: bool\n",
    "    pca_n: int\n",
    "    k_grid: type\n",
    "    conf_threshold: float\n",
    "    random_state: int\n",
    "    tau: float\n",
    "\n",
    "    experiment: str\n",
    "    run_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5328135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_file_path=CONFIG_FILE_PATH):\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_categorisation_config(self) -> CategorisationConfig:\n",
    "        config = self.config.data_categorisation\n",
    "        model_config = config.model\n",
    "        params_config = config.params\n",
    "        mlflow_config = config.mlflow\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_categorisation_config = CategorisationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            source = config.source,\n",
    "            seeds = config.seeds,\n",
    "\n",
    "            model_name = model_config.model_name,\n",
    "            batch_size = model_config.batch_size,\n",
    "            normalise = model_config.normalise,\n",
    "\n",
    "            use_pca = params_config.use_pca,\n",
    "            pca_n = params_config.pca_n,\n",
    "            k_grid = params_config.k_grid,\n",
    "            conf_threshold = params_config.conf_threshold,\n",
    "            random_state = params_config.random_state,\n",
    "            tau = params_config.tau,\n",
    "\n",
    "            experiment = mlflow_config.experiment,\n",
    "            run_name = mlflow_config.run_name\n",
    "        )\n",
    "        return data_categorisation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f711f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCategorisation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def init(self, data_categorisation_config:CategorisationConfig):\n",
    "        self.data_categorisation_config = data_categorisation_config\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        enc = SentenceTransformer(self.data_categorisation_config.model_name)\n",
    "        E = enc.encode(texts, batch_size=self.data_categorisation_config.batch_size, show_progress_bar=False, normalize_embeddings=self.data_categorisation_config.normalise)\n",
    "        return np.asarray(E, dtype=np.float32)\n",
    "\n",
    "    \n",
    "    def ctfidf_top_terms(self, texts_by_cluster, top_n=6, ngram=(1,3)):\n",
    "        docs = [\" \".join(t) for t in texts_by_cluster]\n",
    "        cv = CountVectorizer(ngram_range=ngram, min_df=1)\n",
    "        X = cv.fit_transform(docs)\n",
    "        idf = TfidfTransformer(use_idf=True, norm=None).fit(X).idf_\n",
    "        ctfidf = X.multiply(idf).tocsr()\n",
    "        tokens = np.array(cv.get_feature_names_out())\n",
    "        out = []\n",
    "        for i in range(ctfidf.shape[0]):\n",
    "            row = ctfidf[i].toarray().ravel()\n",
    "            idx = row.argsort()[-top_n:][::-1]\n",
    "            out.append(tokens[idx].tolist())\n",
    "        return out    \n",
    "    \n",
    "    \n",
    "    def l2(self, X: np.ndarray):\n",
    "        return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    def load_seeds(self) -> dict:\n",
    "        js_path = Path(self.data_categorisation_config.seeds)\n",
    "        text = js_path.read_text(encoding=\"utf-8-sig\")\n",
    "        \n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        fixed = re.sub(r'([\\]\\}])\\s*(\")', r'\\1,\\n\\2', text)\n",
    "\n",
    "        try:\n",
    "            data = json.loads(fixed)\n",
    "            p_fixed = js_path.with_suffix(\".fixed.json\")\n",
    "            p_fixed.write_text(fixed, encoding=\"utf-8\")\n",
    "            print(f\"[fixed] wrote {p_fixed}\")\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "    \n",
    "    def make_seed_centroids(self, seeds: dict):\n",
    "        items = {k: v for k, v in seeds.items()}\n",
    "        labels = list(items.keys())\n",
    "        centroids = []\n",
    "        for lbl in labels:\n",
    "            phrases = items[lbl]\n",
    "            E_seed = self.embed_texts(phrases)\n",
    "            centroids.append(E_seed.mean(axis=0, keepdims=False))\n",
    "        C = self.l2(np.vstack(centroids))\n",
    "        return C, labels\n",
    "\n",
    "    def assign_to_seeds(self, texts: list[str], C: np.ndarray, labels: list[str]) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        try:\n",
    "            E = self.embed_texts(texts)\n",
    "            E = self.l2(E)\n",
    "            S = E @ C.T\n",
    "            tau = self.data_categorisation_config.tau\n",
    "            idx = S.argmax(axis=1)\n",
    "            best_sim = S[np.arange(len(E)), idx]\n",
    "            chosen = np.array(labels, dtype=object)[idx]\n",
    "            chosen = np.where(best_sim >= tau, chosen, \"Other\")\n",
    "            return chosen, best_sim, S\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "            \n",
    "\n",
    "    def run_seed_assignment(self):\n",
    "        config = ConfigurationManager()\n",
    "        self.data_categorisation_config = config.get_data_categorisation_config()\n",
    "        tau = self.data_categorisation_config.tau\n",
    "\n",
    "        mlflow.set_experiment(self.data_categorisation_config.experiment)\n",
    "        with mlflow.start_run(run_name=f\"{self.data_categorisation_config.run_name}__seed-centroids\"):\n",
    "            mlflow.log_param(\"mode\", \"seed-centroids\")\n",
    "            mlflow.log_param(\"model_name\", self.data_categorisation_config.model_name)\n",
    "            mlflow.log_param(\"tau\", tau)\n",
    "            mlflow.log_param(\"seeds_path\", str(self.data_categorisation_config.seeds))\n",
    "\n",
    "            df = pd.read_csv(self.data_categorisation_config.source)\n",
    "            if \"Item Name\" not in df.columns:\n",
    "                raise ValueError(\"Missing 'Item Name' column in source data.\")\n",
    "            texts = (df[\"Item Name\"].fillna(\"\")\n",
    "                    .astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).tolist())\n",
    "\n",
    "            seeds = self.load_seeds()\n",
    "            mlflow.log_param(\"n_predef_clusters\", int(len(seeds)))\n",
    "            C, labels = self.make_seed_centroids(seeds)\n",
    "\n",
    "            chosen, best_sim, _ = self.assign_to_seeds(texts, C, labels)\n",
    "\n",
    "            label_to_id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "            cluster_id = np.array([label_to_id.get(lbl, -1) for lbl in chosen], dtype=int)\n",
    "\n",
    "            keep_cols = [c for c in (\"Item ID\", \"Item Name\") if c in df.columns]\n",
    "            if \"Item ID\" not in keep_cols:\n",
    "                df = df.reset_index().rename(columns={\"index\": \"Item ID\"})\n",
    "                keep_cols = [c for c in (\"Item ID\", \"Item Name\") if c in df.columns]\n",
    "            clusters_df = df[keep_cols].copy()\n",
    "            clusters_df[\"cluster_id\"] = cluster_id\n",
    "            clusters_df[\"confidence\"] = best_sim.clip(0, 1)\n",
    "            clusters_df[\"src\"] = \"seed-centroid\"\n",
    "\n",
    "            p_clusters = self.data_categorisation_config.root_dir / \"clusters.csv\"\n",
    "            clusters_df.to_csv(p_clusters, index=False)\n",
    "            mlflow.log_artifact(str(p_clusters))\n",
    "\n",
    "            rows, texts_by_c = [], []\n",
    "            for lbl, cid in label_to_id.items():\n",
    "                mask = (cluster_id == cid)\n",
    "                size = int(mask.sum())\n",
    "                if size > 0:\n",
    "                    rows.append({\"cluster_id\": cid, \"label\": lbl, \"size\": size})\n",
    "                    texts_by_c.append(list(pd.Series(texts)[mask]))\n",
    "\n",
    "            tops = self.ctfidf_top_terms(texts_by_c, top_n=6, ngram=(1,3)) if texts_by_c else []\n",
    "            for r, terms in zip(rows, tops):\n",
    "                r[\"top_terms\"] = terms\n",
    "\n",
    "            labels_df = pd.DataFrame(rows).sort_values(\"size\", ascending=False)\n",
    "            p_labels = self.data_categorisation_config.root_dir / \"cluster_labels.csv\"\n",
    "            labels_df.to_csv(p_labels, index=False)\n",
    "            mlflow.log_artifact(str(p_labels))\n",
    "\n",
    "            coverage = float((cluster_id != -1).mean())\n",
    "            mlflow.log_metric(\"coverage_non_other\", coverage)\n",
    "            mlflow.log_metric(\"n_clusters_effective\", int((labels_df[\"size\"] > 0).sum()))\n",
    "\n",
    "            logger.info(f\"[SEED-ASSIGN] τ={tau:.2f}, coverage={coverage:.1%}\")\n",
    "            logger.info(f\"Saved: {p_clusters}, {p_labels}\")\n",
    "            return p_clusters, p_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0679430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 00:32:45,319: INFO: common: yaml file config/config.yaml is loaded successfully]\n",
      "[2025-08-25 00:32:45,319: INFO: common: created directory at artifacts]\n",
      "[2025-08-25 00:32:45,320: INFO: common: created directory at artifacts/categorisation]\n",
      "[2025-08-25 00:32:45,396: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:32:45,396: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:32:51,109: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:32:51,110: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:32:55,943: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:32:55,943: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:02,126: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:02,127: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:06,203: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:06,204: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:10,825: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:10,826: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:15,475: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:15,475: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:20,259: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:20,260: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:24,492: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:24,492: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:28,962: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:28,963: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[2025-08-25 00:33:34,048: INFO: SentenceTransformer: Use pytorch device_name: mps]\n",
      "[2025-08-25 00:33:34,048: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2]\n",
      "[SEED-ASSIGN] τ=0.25, coverage=100.0%\n",
      "Saved: artifacts/categorisation/clusters.csv, artifacts/categorisation/cluster_labels.csv\n"
     ]
    }
   ],
   "source": [
    "a = DataCategorisation()\n",
    "a = a.run_seed_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c181d9f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3574192917.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmlflow ui\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0cd191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
